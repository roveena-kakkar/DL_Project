{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWih0FXGacH8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Building the initial Convolutional Block\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        nn.init.xavier_uniform_(self.conv.weight,gain=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Building the Inception Block\n",
        "### “#3×3 reduce” and “#5×5 reduce”\n",
        "From Paper - “#3 × 3 reduce” and “#5 × 5 reduce” stands for the number of 1 × 1 filters\n",
        "in the reduction layer used before the 3 × 3 and 5 × 5 convolutions.\n",
        "One can see the number of 1 × 1 filters in the projection layer after the\n",
        "built-in max-pooling in the “pool proj” column.\n",
        "All these reduction/ projection layers use rectified linear (ReLU) activation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num1x1,\n",
        "        num3x3_reduce,\n",
        "        num3x3,\n",
        "        num5x5_reduce,\n",
        "        num5x5,\n",
        "        pool_proj,\n",
        "    ):\n",
        "        super(Inception, self).__init__()\n",
        "\n",
        "        # Four output channel for each parallel block of network\n",
        "        # Note, within Inception the individual blocks are running parallely\n",
        "        # NOT sequentially.\n",
        "        self.block1 = nn.Sequential(\n",
        "            ConvBlock(in_channels, num1x1, kernel_size=1, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            ConvBlock(in_channels, num3x3_reduce, kernel_size=1, stride=1, padding=0),\n",
        "            ConvBlock(num3x3_reduce, num3x3, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ConvBlock(in_channels, num5x5_reduce, kernel_size=1, stride=1, padding=0),\n",
        "            ConvBlock(num5x5_reduce, num5x5, kernel_size=5, stride=1, padding=2),\n",
        "        )\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1, ceil_mode=True),\n",
        "            ConvBlock(in_channels, pool_proj, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Note the different way this forward function\n",
        "        # calculates the output.\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(x)\n",
        "        block3 = self.block3(x)\n",
        "        block4 = self.block4(x)\n",
        "\n",
        "        return torch.cat([block1, block2, block3, block4], 1)\n",
        "\n",
        "\n",
        "class Auxiliary(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(Auxiliary, self).__init__()\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
        "        nn.init.xavier_uniform_(self.conv.weight,gain=2)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 1024)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool(x)\n",
        "\n",
        "        out = self.conv(out)\n",
        "        out = self.activation(out)\n",
        "        print('out shape is  ', out.shape)\n",
        "        # out shape is  torch.Size([2, 128, 4, 4])\n",
        "\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = ConvBlock(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.pool1 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv2 = ConvBlock(64, 64, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv3 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "        self.inception3A = Inception(\n",
        "            in_channels=192,\n",
        "            num1x1=64,\n",
        "            num3x3_reduce=96,\n",
        "            num3x3=128,\n",
        "            num5x5_reduce=16,\n",
        "            num5x5=32,\n",
        "            pool_proj=32,\n",
        "        )\n",
        "        self.inception3B = Inception(\n",
        "            in_channels=256,\n",
        "            num1x1=128,\n",
        "            num3x3_reduce=128,\n",
        "            num3x3=192,\n",
        "            num5x5_reduce=32,\n",
        "            num5x5=96,\n",
        "            pool_proj=64,\n",
        "        )\n",
        "        self.pool4 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "        self.inception4A = Inception(\n",
        "            in_channels=480,\n",
        "            num1x1=192,\n",
        "            num3x3_reduce=96,\n",
        "            num3x3=208,\n",
        "            num5x5_reduce=16,\n",
        "            num5x5=48,\n",
        "            pool_proj=64,\n",
        "        )\n",
        "        self.inception4B = Inception(\n",
        "            in_channels=512,\n",
        "            num1x1=160,\n",
        "            num3x3_reduce=112,\n",
        "            num3x3=224,\n",
        "            num5x5_reduce=24,\n",
        "            num5x5=64,\n",
        "            pool_proj=64,\n",
        "        )\n",
        "        self.inception4C = Inception(\n",
        "            in_channels=512,\n",
        "            num1x1=128,\n",
        "            num3x3_reduce=128,\n",
        "            num3x3=256,\n",
        "            num5x5_reduce=24,\n",
        "            num5x5=64,\n",
        "            pool_proj=64,\n",
        "        )\n",
        "        self.inception4D = Inception(\n",
        "            in_channels=512,\n",
        "            num1x1=112,\n",
        "            num3x3_reduce=144,\n",
        "            num3x3=288,\n",
        "            num5x5_reduce=32,\n",
        "            num5x5=64,\n",
        "            pool_proj=64,\n",
        "        )\n",
        "        self.inception4E = Inception(\n",
        "            in_channels=528,\n",
        "            num1x1=256,\n",
        "            num3x3_reduce=160,\n",
        "            num3x3=320,\n",
        "            num5x5_reduce=32,\n",
        "            num5x5=128,\n",
        "            pool_proj=128,\n",
        "        )\n",
        "        self.pool5 = nn.MaxPool2d(3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "        self.inception5A = Inception(\n",
        "            in_channels=832,\n",
        "            num1x1=256,\n",
        "            num3x3_reduce=160,\n",
        "            num3x3=320,\n",
        "            num5x5_reduce=32,\n",
        "            num5x5=128,\n",
        "            pool_proj=128,\n",
        "        )\n",
        "        self.inception5B = Inception(\n",
        "            in_channels=832,\n",
        "            num1x1=384,\n",
        "            num3x3_reduce=192,\n",
        "            num3x3=384,\n",
        "            num5x5_reduce=48,\n",
        "            num5x5=128,\n",
        "            pool_proj=128,\n",
        "        )\n",
        "        self.pool6 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "        self.aux4A = Auxiliary(512, num_classes)\n",
        "        self.aux4D = Auxiliary(528, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.pool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.pool3(out)\n",
        "        out = self.inception3A(out)\n",
        "        out = self.inception3B(out)\n",
        "        out = self.pool4(out)\n",
        "        out = self.inception4A(out)\n",
        "\n",
        "        aux1 = self.aux4A(out)\n",
        "\n",
        "        out = self.inception4B(out)\n",
        "        out = self.inception4C(out)\n",
        "        out = self.inception4D(out)\n",
        "\n",
        "        aux2 = self.aux4D(out)\n",
        "\n",
        "        out = self.inception4E(out)\n",
        "        out = self.pool5(out)\n",
        "        out = self.inception5A(out)\n",
        "        out = self.inception5B(out)\n",
        "        out = self.pool6(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, aux1, aux2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer):\n",
        "    EPOCHS = 15\n",
        "    train_samples_num = 45000\n",
        "    val_samples_num = 5000\n",
        "\n",
        "    train_epoch_loss_history, val_epoch_loss_history = [], []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        train_running_loss = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        model.train().cuda()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            \"\"\" for every mini-batch during the training phase, we\n",
        "            typically want to explicitly set the gradients\n",
        "            to zero before starting to do backpropragation \"\"\"\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Start the forward pass\n",
        "            prediction0, aux_pred_1, aux_pred_2 = model(inputs)\n",
        "\n",
        "            # Compute the loss.\n",
        "            real_loss = criterion(prediction0, labels)\n",
        "            aux_loss_1 = criterion(aux_pred_1, labels)\n",
        "            aux_loss_2 = criterion(aux_pred_2, labels)\n",
        "\n",
        "            loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2\n",
        "\n",
        "            # do backpropagation and update weights with step()# Backward pass.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the running corrects\n",
        "            _, predicted = torch.max(prediction0.data, 1)\n",
        "\n",
        "            correct_train += (predicted == labels).float().sum().item()\n",
        "\n",
        "            \"\"\" Compute batch loss\n",
        "            multiply each average batch loss with batch-length.\n",
        "            The batch-length is inputs.size(0) which gives the number total images in each batch.\n",
        "            Essentially I am un-averaging the previously calculated Loss \"\"\"\n",
        "            train_running_loss += loss.data.item() * inputs.shape[0]\n",
        "\n",
        "        train_epoch_loss = train_running_loss / train_samples_num\n",
        "\n",
        "        train_epoch_loss_history.append(train_epoch_loss)\n",
        "\n",
        "        train_acc = correct_train / train_samples_num\n",
        "\n",
        "        val_loss = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        model.eval().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass.\n",
        "                prediction0, aux_pred_1, aux_pred_2 = model(inputs)\n",
        "\n",
        "                # Compute the loss.\n",
        "                real_loss = criterion(prediction0, labels)\n",
        "                aux_loss_1 = criterion(aux_pred_1, labels)\n",
        "                aux_loss_2 = criterion(aux_pred_2, labels)\n",
        "\n",
        "                loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2\n",
        "\n",
        "                # Compute training accuracy.\n",
        "                _, predicted = torch.max(prediction0.data, 1)\n",
        "                correct_val += (predicted == labels).float().sum().item()\n",
        "\n",
        "                # Compute batch loss.\n",
        "                val_loss += loss.data.item() * inputs.shape[0]\n",
        "\n",
        "            val_loss /= val_samples_num\n",
        "            val_epoch_loss_history.append(val_loss)\n",
        "            val_acc = correct_val / val_samples_num\n",
        "\n",
        "        info = \"[For Epoch {}/{}]: train-loss = {:0.5f} | train-acc = {:0.3f} | val-loss = {:0.5f} | val-acc = {:0.3f}\"\n",
        "\n",
        "        print(\n",
        "            info.format(\n",
        "                epoch + 1, EPOCHS, train_epoch_loss, train_acc, val_loss, val_acc\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        torch.save(\n",
        "            model.state_dict(), \"/content/sample_data/checkpoint{}\".format(epoch + 1)\n",
        "        )\n",
        "\n",
        "    torch.save(model.state_dict(), \"/content/sample_data/googlenet_model\")\n",
        "\n",
        "    return train_epoch_loss_history, val_epoch_loss_history"
      ],
      "metadata": {
        "id": "QRJrm_z-jVNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1wJcr6VkB2n",
        "outputId": "1f630ab1-ca9b-4dae-c283-7c895dd7609a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ntWAfNIWkIrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogLeNet()\n",
        "model.to(device)\n",
        "summary(model, (3, 96, 96))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51iLBtl6kWBB",
        "outputId": "160883ab-b55d-449c-d231-74d1d1ce43b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out shape is   torch.Size([2, 128, 4, 4])\n",
            "out shape is   torch.Size([2, 128, 4, 4])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]           9,472\n",
            "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
            "              ReLU-3           [-1, 64, 48, 48]               0\n",
            "         ConvBlock-4           [-1, 64, 48, 48]               0\n",
            "         MaxPool2d-5           [-1, 64, 24, 24]               0\n",
            "            Conv2d-6           [-1, 64, 24, 24]           4,160\n",
            "       BatchNorm2d-7           [-1, 64, 24, 24]             128\n",
            "              ReLU-8           [-1, 64, 24, 24]               0\n",
            "         ConvBlock-9           [-1, 64, 24, 24]               0\n",
            "           Conv2d-10          [-1, 192, 24, 24]         110,784\n",
            "      BatchNorm2d-11          [-1, 192, 24, 24]             384\n",
            "             ReLU-12          [-1, 192, 24, 24]               0\n",
            "        ConvBlock-13          [-1, 192, 24, 24]               0\n",
            "        MaxPool2d-14          [-1, 192, 12, 12]               0\n",
            "           Conv2d-15           [-1, 64, 12, 12]          12,352\n",
            "      BatchNorm2d-16           [-1, 64, 12, 12]             128\n",
            "             ReLU-17           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-18           [-1, 64, 12, 12]               0\n",
            "           Conv2d-19           [-1, 96, 12, 12]          18,528\n",
            "      BatchNorm2d-20           [-1, 96, 12, 12]             192\n",
            "             ReLU-21           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-22           [-1, 96, 12, 12]               0\n",
            "           Conv2d-23          [-1, 128, 12, 12]         110,720\n",
            "      BatchNorm2d-24          [-1, 128, 12, 12]             256\n",
            "             ReLU-25          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-26          [-1, 128, 12, 12]               0\n",
            "           Conv2d-27           [-1, 16, 12, 12]           3,088\n",
            "      BatchNorm2d-28           [-1, 16, 12, 12]              32\n",
            "             ReLU-29           [-1, 16, 12, 12]               0\n",
            "        ConvBlock-30           [-1, 16, 12, 12]               0\n",
            "           Conv2d-31           [-1, 32, 12, 12]          12,832\n",
            "      BatchNorm2d-32           [-1, 32, 12, 12]              64\n",
            "             ReLU-33           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-34           [-1, 32, 12, 12]               0\n",
            "        MaxPool2d-35          [-1, 192, 12, 12]               0\n",
            "           Conv2d-36           [-1, 32, 12, 12]           6,176\n",
            "      BatchNorm2d-37           [-1, 32, 12, 12]              64\n",
            "             ReLU-38           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-39           [-1, 32, 12, 12]               0\n",
            "        Inception-40          [-1, 256, 12, 12]               0\n",
            "           Conv2d-41          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-42          [-1, 128, 12, 12]             256\n",
            "             ReLU-43          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-44          [-1, 128, 12, 12]               0\n",
            "           Conv2d-45          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-46          [-1, 128, 12, 12]             256\n",
            "             ReLU-47          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-48          [-1, 128, 12, 12]               0\n",
            "           Conv2d-49          [-1, 192, 12, 12]         221,376\n",
            "      BatchNorm2d-50          [-1, 192, 12, 12]             384\n",
            "             ReLU-51          [-1, 192, 12, 12]               0\n",
            "        ConvBlock-52          [-1, 192, 12, 12]               0\n",
            "           Conv2d-53           [-1, 32, 12, 12]           8,224\n",
            "      BatchNorm2d-54           [-1, 32, 12, 12]              64\n",
            "             ReLU-55           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-56           [-1, 32, 12, 12]               0\n",
            "           Conv2d-57           [-1, 96, 12, 12]          76,896\n",
            "      BatchNorm2d-58           [-1, 96, 12, 12]             192\n",
            "             ReLU-59           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-60           [-1, 96, 12, 12]               0\n",
            "        MaxPool2d-61          [-1, 256, 12, 12]               0\n",
            "           Conv2d-62           [-1, 64, 12, 12]          16,448\n",
            "      BatchNorm2d-63           [-1, 64, 12, 12]             128\n",
            "             ReLU-64           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-65           [-1, 64, 12, 12]               0\n",
            "        Inception-66          [-1, 480, 12, 12]               0\n",
            "        MaxPool2d-67            [-1, 480, 6, 6]               0\n",
            "           Conv2d-68            [-1, 192, 6, 6]          92,352\n",
            "      BatchNorm2d-69            [-1, 192, 6, 6]             384\n",
            "             ReLU-70            [-1, 192, 6, 6]               0\n",
            "        ConvBlock-71            [-1, 192, 6, 6]               0\n",
            "           Conv2d-72             [-1, 96, 6, 6]          46,176\n",
            "      BatchNorm2d-73             [-1, 96, 6, 6]             192\n",
            "             ReLU-74             [-1, 96, 6, 6]               0\n",
            "        ConvBlock-75             [-1, 96, 6, 6]               0\n",
            "           Conv2d-76            [-1, 208, 6, 6]         179,920\n",
            "      BatchNorm2d-77            [-1, 208, 6, 6]             416\n",
            "             ReLU-78            [-1, 208, 6, 6]               0\n",
            "        ConvBlock-79            [-1, 208, 6, 6]               0\n",
            "           Conv2d-80             [-1, 16, 6, 6]           7,696\n",
            "      BatchNorm2d-81             [-1, 16, 6, 6]              32\n",
            "             ReLU-82             [-1, 16, 6, 6]               0\n",
            "        ConvBlock-83             [-1, 16, 6, 6]               0\n",
            "           Conv2d-84             [-1, 48, 6, 6]          19,248\n",
            "      BatchNorm2d-85             [-1, 48, 6, 6]              96\n",
            "             ReLU-86             [-1, 48, 6, 6]               0\n",
            "        ConvBlock-87             [-1, 48, 6, 6]               0\n",
            "        MaxPool2d-88            [-1, 480, 6, 6]               0\n",
            "           Conv2d-89             [-1, 64, 6, 6]          30,784\n",
            "      BatchNorm2d-90             [-1, 64, 6, 6]             128\n",
            "             ReLU-91             [-1, 64, 6, 6]               0\n",
            "        ConvBlock-92             [-1, 64, 6, 6]               0\n",
            "        Inception-93            [-1, 512, 6, 6]               0\n",
            "AdaptiveAvgPool2d-94            [-1, 512, 4, 4]               0\n",
            "           Conv2d-95            [-1, 128, 4, 4]          65,664\n",
            "             ReLU-96            [-1, 128, 4, 4]               0\n",
            "           Linear-97                 [-1, 1024]       2,098,176\n",
            "             ReLU-98                 [-1, 1024]               0\n",
            "          Dropout-99                 [-1, 1024]               0\n",
            "          Linear-100                   [-1, 10]          10,250\n",
            "       Auxiliary-101                   [-1, 10]               0\n",
            "          Conv2d-102            [-1, 160, 6, 6]          82,080\n",
            "     BatchNorm2d-103            [-1, 160, 6, 6]             320\n",
            "            ReLU-104            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-105            [-1, 160, 6, 6]               0\n",
            "          Conv2d-106            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-107            [-1, 112, 6, 6]             224\n",
            "            ReLU-108            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-109            [-1, 112, 6, 6]               0\n",
            "          Conv2d-110            [-1, 224, 6, 6]         226,016\n",
            "     BatchNorm2d-111            [-1, 224, 6, 6]             448\n",
            "            ReLU-112            [-1, 224, 6, 6]               0\n",
            "       ConvBlock-113            [-1, 224, 6, 6]               0\n",
            "          Conv2d-114             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-115             [-1, 24, 6, 6]              48\n",
            "            ReLU-116             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-117             [-1, 24, 6, 6]               0\n",
            "          Conv2d-118             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-119             [-1, 64, 6, 6]             128\n",
            "            ReLU-120             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-121             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-122            [-1, 512, 6, 6]               0\n",
            "          Conv2d-123             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-124             [-1, 64, 6, 6]             128\n",
            "            ReLU-125             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-126             [-1, 64, 6, 6]               0\n",
            "       Inception-127            [-1, 512, 6, 6]               0\n",
            "          Conv2d-128            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-129            [-1, 128, 6, 6]             256\n",
            "            ReLU-130            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-131            [-1, 128, 6, 6]               0\n",
            "          Conv2d-132            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-133            [-1, 128, 6, 6]             256\n",
            "            ReLU-134            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-135            [-1, 128, 6, 6]               0\n",
            "          Conv2d-136            [-1, 256, 6, 6]         295,168\n",
            "     BatchNorm2d-137            [-1, 256, 6, 6]             512\n",
            "            ReLU-138            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-139            [-1, 256, 6, 6]               0\n",
            "          Conv2d-140             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-141             [-1, 24, 6, 6]              48\n",
            "            ReLU-142             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-143             [-1, 24, 6, 6]               0\n",
            "          Conv2d-144             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-145             [-1, 64, 6, 6]             128\n",
            "            ReLU-146             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-147             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-148            [-1, 512, 6, 6]               0\n",
            "          Conv2d-149             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-150             [-1, 64, 6, 6]             128\n",
            "            ReLU-151             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-152             [-1, 64, 6, 6]               0\n",
            "       Inception-153            [-1, 512, 6, 6]               0\n",
            "          Conv2d-154            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-155            [-1, 112, 6, 6]             224\n",
            "            ReLU-156            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-157            [-1, 112, 6, 6]               0\n",
            "          Conv2d-158            [-1, 144, 6, 6]          73,872\n",
            "     BatchNorm2d-159            [-1, 144, 6, 6]             288\n",
            "            ReLU-160            [-1, 144, 6, 6]               0\n",
            "       ConvBlock-161            [-1, 144, 6, 6]               0\n",
            "          Conv2d-162            [-1, 288, 6, 6]         373,536\n",
            "     BatchNorm2d-163            [-1, 288, 6, 6]             576\n",
            "            ReLU-164            [-1, 288, 6, 6]               0\n",
            "       ConvBlock-165            [-1, 288, 6, 6]               0\n",
            "          Conv2d-166             [-1, 32, 6, 6]          16,416\n",
            "     BatchNorm2d-167             [-1, 32, 6, 6]              64\n",
            "            ReLU-168             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-169             [-1, 32, 6, 6]               0\n",
            "          Conv2d-170             [-1, 64, 6, 6]          51,264\n",
            "     BatchNorm2d-171             [-1, 64, 6, 6]             128\n",
            "            ReLU-172             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-173             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-174            [-1, 512, 6, 6]               0\n",
            "          Conv2d-175             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-176             [-1, 64, 6, 6]             128\n",
            "            ReLU-177             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-178             [-1, 64, 6, 6]               0\n",
            "       Inception-179            [-1, 528, 6, 6]               0\n",
            "AdaptiveAvgPool2d-180            [-1, 528, 4, 4]               0\n",
            "          Conv2d-181            [-1, 128, 4, 4]          67,712\n",
            "            ReLU-182            [-1, 128, 4, 4]               0\n",
            "          Linear-183                 [-1, 1024]       2,098,176\n",
            "            ReLU-184                 [-1, 1024]               0\n",
            "         Dropout-185                 [-1, 1024]               0\n",
            "          Linear-186                   [-1, 10]          10,250\n",
            "       Auxiliary-187                   [-1, 10]               0\n",
            "          Conv2d-188            [-1, 256, 6, 6]         135,424\n",
            "     BatchNorm2d-189            [-1, 256, 6, 6]             512\n",
            "            ReLU-190            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-191            [-1, 256, 6, 6]               0\n",
            "          Conv2d-192            [-1, 160, 6, 6]          84,640\n",
            "     BatchNorm2d-193            [-1, 160, 6, 6]             320\n",
            "            ReLU-194            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-195            [-1, 160, 6, 6]               0\n",
            "          Conv2d-196            [-1, 320, 6, 6]         461,120\n",
            "     BatchNorm2d-197            [-1, 320, 6, 6]             640\n",
            "            ReLU-198            [-1, 320, 6, 6]               0\n",
            "       ConvBlock-199            [-1, 320, 6, 6]               0\n",
            "          Conv2d-200             [-1, 32, 6, 6]          16,928\n",
            "     BatchNorm2d-201             [-1, 32, 6, 6]              64\n",
            "            ReLU-202             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-203             [-1, 32, 6, 6]               0\n",
            "          Conv2d-204            [-1, 128, 6, 6]         102,528\n",
            "     BatchNorm2d-205            [-1, 128, 6, 6]             256\n",
            "            ReLU-206            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-207            [-1, 128, 6, 6]               0\n",
            "       MaxPool2d-208            [-1, 528, 6, 6]               0\n",
            "          Conv2d-209            [-1, 128, 6, 6]          67,712\n",
            "     BatchNorm2d-210            [-1, 128, 6, 6]             256\n",
            "            ReLU-211            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-212            [-1, 128, 6, 6]               0\n",
            "       Inception-213            [-1, 832, 6, 6]               0\n",
            "       MaxPool2d-214            [-1, 832, 3, 3]               0\n",
            "          Conv2d-215            [-1, 256, 3, 3]         213,248\n",
            "     BatchNorm2d-216            [-1, 256, 3, 3]             512\n",
            "            ReLU-217            [-1, 256, 3, 3]               0\n",
            "       ConvBlock-218            [-1, 256, 3, 3]               0\n",
            "          Conv2d-219            [-1, 160, 3, 3]         133,280\n",
            "     BatchNorm2d-220            [-1, 160, 3, 3]             320\n",
            "            ReLU-221            [-1, 160, 3, 3]               0\n",
            "       ConvBlock-222            [-1, 160, 3, 3]               0\n",
            "          Conv2d-223            [-1, 320, 3, 3]         461,120\n",
            "     BatchNorm2d-224            [-1, 320, 3, 3]             640\n",
            "            ReLU-225            [-1, 320, 3, 3]               0\n",
            "       ConvBlock-226            [-1, 320, 3, 3]               0\n",
            "          Conv2d-227             [-1, 32, 3, 3]          26,656\n",
            "     BatchNorm2d-228             [-1, 32, 3, 3]              64\n",
            "            ReLU-229             [-1, 32, 3, 3]               0\n",
            "       ConvBlock-230             [-1, 32, 3, 3]               0\n",
            "          Conv2d-231            [-1, 128, 3, 3]         102,528\n",
            "     BatchNorm2d-232            [-1, 128, 3, 3]             256\n",
            "            ReLU-233            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-234            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-235            [-1, 832, 3, 3]               0\n",
            "          Conv2d-236            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-237            [-1, 128, 3, 3]             256\n",
            "            ReLU-238            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-239            [-1, 128, 3, 3]               0\n",
            "       Inception-240            [-1, 832, 3, 3]               0\n",
            "          Conv2d-241            [-1, 384, 3, 3]         319,872\n",
            "     BatchNorm2d-242            [-1, 384, 3, 3]             768\n",
            "            ReLU-243            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-244            [-1, 384, 3, 3]               0\n",
            "          Conv2d-245            [-1, 192, 3, 3]         159,936\n",
            "     BatchNorm2d-246            [-1, 192, 3, 3]             384\n",
            "            ReLU-247            [-1, 192, 3, 3]               0\n",
            "       ConvBlock-248            [-1, 192, 3, 3]               0\n",
            "          Conv2d-249            [-1, 384, 3, 3]         663,936\n",
            "     BatchNorm2d-250            [-1, 384, 3, 3]             768\n",
            "            ReLU-251            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-252            [-1, 384, 3, 3]               0\n",
            "          Conv2d-253             [-1, 48, 3, 3]          39,984\n",
            "     BatchNorm2d-254             [-1, 48, 3, 3]              96\n",
            "            ReLU-255             [-1, 48, 3, 3]               0\n",
            "       ConvBlock-256             [-1, 48, 3, 3]               0\n",
            "          Conv2d-257            [-1, 128, 3, 3]         153,728\n",
            "     BatchNorm2d-258            [-1, 128, 3, 3]             256\n",
            "            ReLU-259            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-260            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-261            [-1, 832, 3, 3]               0\n",
            "          Conv2d-262            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-263            [-1, 128, 3, 3]             256\n",
            "            ReLU-264            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-265            [-1, 128, 3, 3]               0\n",
            "       Inception-266           [-1, 1024, 3, 3]               0\n",
            "AdaptiveAvgPool2d-267           [-1, 1024, 1, 1]               0\n",
            "         Dropout-268                 [-1, 1024]               0\n",
            "          Linear-269                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 10,348,590\n",
            "Trainable params: 10,348,590\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 22.05\n",
            "Params size (MB): 39.48\n",
            "Estimated Total Size (MB): 61.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar_dataloader():\n",
        "    \n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "            \n",
        "    # Input Data in Local Machine\n",
        "    # train_dataset = datasets.CIFAR10('../input_data', train=True, download=True, transform=transform)\n",
        "    # test_dataset = datasets.CIFAR10('../input_data', train=False, download=True, transform=transform)\n",
        "    \n",
        "    # Input Data in Google Drive\n",
        "    train_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=True, download=True, transform=transform)\n",
        "    \n",
        "    test_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split dataset into training set and validation set.\n",
        "    train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
        "    \n",
        "    print(\"Image shape of a random sample image : {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
        "    \n",
        "    print(\"Training Set:   {} images\".format(len(train_dataset)))\n",
        "    print(\"Validation Set:   {} images\".format(len(val_dataset)))\n",
        "    print(\"Test Set:       {} images\".format(len(test_dataset)))\n",
        "    \n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Generate dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "KJaXLK6jkd98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = cifar_dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGLqei4Bk1rZ",
        "outputId": "623f899f-6798-442f-9a10-36431ede9b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set:   45000 images\n",
            "Validation Set:   5000 images\n",
            "Test Set:       10000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "EO4nm97vk2ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_loss_history, val_epoch_loss_history = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "EPOCHS=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "plt.plot(EPOCHS,train_epoch_loss_history,label=\"training loss\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss Vs Epochs\")\n",
        "plt.plot(EPOCHS,val_epoch_loss_history,label=\"validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "        "
      ],
      "metadata": {
        "id": "fdtG7oN-k5WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_samples = 10000\n",
        "correct = 0 \n",
        "\n",
        "model.eval().cuda()\n",
        "\n",
        "with  torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Make predictions.\n",
        "        prediction, _, _ = model(inputs)\n",
        "\n",
        "        # Retrieve predictions indexes.\n",
        "        _, predicted_class = torch.max(prediction.data, 1)\n",
        "\n",
        "        # Compute number of correct predictions.\n",
        "        correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = correct / num_test_samples\n",
        "\n",
        "print('Test accuracy: {}'.format(test_accuracy))"
      ],
      "metadata": {
        "id": "LfyGeB-ZlCuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}